{"cells":[{"cell_type":"markdown","metadata":{"id":"AWGzucuFfbBn"},"source":["\n","# Intro to LangChain\n","\n","LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n","\n","The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n","\n","* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n","\n","* **LLMs**: Large language models like GPT-3, BLOOM, etc\n","\n","* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n","\n","* **Memory**: Short-term memory, long-term memory."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"r-ryCeG_f_GC","executionInfo":{"status":"ok","timestamp":1697273870691,"user_tz":-330,"elapsed":4854,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["!pip install -qU langchain"]},{"cell_type":"markdown","metadata":{"id":"mNaXrEPOhbuL"},"source":["# Using LLMs in LangChain\n","\n","LangChain supports several LLM providers, like Hugging Face and OpenAI.\n","\n","Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n","\n","## Hugging Face\n","\n","We first need to install additional prerequisite libraries:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWA15ZkVjg80","outputId":"b06a11ac-2bfe-403d-e01a-40cbcebe1b2b","executionInfo":{"status":"ok","timestamp":1697273875818,"user_tz":-330,"elapsed":5132,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m297.0/302.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qU huggingface_hub"]},{"cell_type":"markdown","metadata":{"id":"m-whfR5Tjf1O"},"source":["For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sRGTytxCjKaW","executionInfo":{"status":"ok","timestamp":1697273875818,"user_tz":-330,"elapsed":6,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["import os\n","\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"]},{"cell_type":"markdown","metadata":{"id":"exAl3iQgnAra"},"source":["We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n","\n","_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7yubiSJhIfs","outputId":"ed1d66a9-6e61-4ff1-9c4e-8c7baeea6472","executionInfo":{"status":"ok","timestamp":1697273877377,"user_tz":-330,"elapsed":1563,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n","  warnings.warn(warning_message, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":[" The Cowboys.\n"]}],"source":["from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n","\n","# initialize HF LLM\n","flan_t5 = HuggingFaceHub(\n","    repo_id=\"bigscience/bloom-560m\",\n","    model_kwargs={\"temperature\":1e-10}\n",")\n","\n","# build prompt template for simple question-answering\n","template = \"\"\"Question: {question}\n","\n","Answer: \"\"\"\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n","\n","llm_chain = LLMChain(\n","    prompt=prompt,\n","    llm=flan_t5\n",")\n","\n","question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n","\n","print(llm_chain.run(question))"]},{"cell_type":"markdown","metadata":{"id":"DXroZSjCKxa2"},"source":["If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jNZgxSIJsXj","outputId":"80f39113-c706-47cb-faeb-7779f5711acf","executionInfo":{"status":"ok","timestamp":1697273877753,"user_tz":-330,"elapsed":381,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LLMResult(generations=[[Generation(text=' The Cowboys.')], [Generation(text=' 6')], [Generation(text=' The 12th person on the moon was')], [Generation(text=' The blade of grass has a')]], llm_output=None, run=[RunInfo(run_id=UUID('672124d0-473f-4f1c-a0dc-e056cd26fdb2')), RunInfo(run_id=UUID('2c145359-f7ec-49df-a8b5-544590e81d01')), RunInfo(run_id=UUID('c3e3bd85-48ae-4527-8ccc-f357b2bc2d3f')), RunInfo(run_id=UUID('5143a9be-ed19-49d3-8dea-fc809f90296b'))])"]},"metadata":{},"execution_count":10}],"source":["qs = [\n","    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n","    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n","    {'question': \"Who was the 12th person on the moon?\"},\n","    {'question': \"How many eyes does a blade of grass have?\"}\n","]\n","res = llm_chain.generate(qs)\n","res"]},{"cell_type":"markdown","metadata":{"id":"7zoxlXHYLQix"},"source":["It is a LLM, so we can try feeding in all questions at once:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b96WIvouLQ-7","outputId":"7fe1066d-539a-49a9-d0ff-29bb891129cb","executionInfo":{"status":"ok","timestamp":1697273877753,"user_tz":-330,"elapsed":7,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The\n"]}],"source":["multi_template = \"\"\"Answer the following questions one at a time.\n","\n","Questions:\n","{questions}\n","\n","Answers:\n","\"\"\"\n","long_prompt = PromptTemplate(\n","    template=multi_template,\n","    input_variables=[\"questions\"]\n",")\n","\n","llm_chain = LLMChain(\n","    prompt=long_prompt,\n","    llm=flan_t5\n",")\n","\n","qs_str = (\n","    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n","    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n","    \"Who was the 12th person on the moon?\" +\n","    \"How many eyes does a blade of grass have?\"\n",")\n","\n","print(llm_chain.run(qs_str))"]},{"cell_type":"markdown","metadata":{"id":"y99CMKSbOqBy"},"source":["But with this model it doesn't work too well, we'll see this approach works better with different models soon."]},{"cell_type":"markdown","metadata":{"id":"YpdXG9YtzrLJ"},"source":["## OpenAI\n","\n","Start by installing additional prerequisites:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHo2YRHPDgHH","outputId":"bffca550-5f07-46aa-feaa-cc07c941774f","executionInfo":{"status":"ok","timestamp":1697273884139,"user_tz":-330,"elapsed":6390,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/77.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qU openai"]},{"cell_type":"markdown","metadata":{"id":"0fOo9qQvDgkz"},"source":["We can also use OpenAI's generative models. The process is similar, we need to\n","give our API key which can be retrieved by signing up for an account on the\n","[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"deWmOJecfbBr","executionInfo":{"status":"ok","timestamp":1697273884139,"user_tz":-330,"elapsed":10,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = ''"]},{"cell_type":"markdown","metadata":{"id":"CU4xirWX-Ds4"},"source":["If using OpenAI via Azure you should also set:\n","\n","```python\n","os.environ['OPENAI_API_TYPE'] = 'azure'\n","# API version to use (Azure has several)\n","os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n","# base URL for your Azure OpenAI resource\n","os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'\n","```"]},{"cell_type":"markdown","metadata":{"id":"2AWnaTCP0Ryg"},"source":["Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ZhQSDoYe0ly4","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"error","timestamp":1697273884139,"user_tz":-330,"elapsed":9,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}},"outputId":"483363db-68c3-4dcb-d2b9-2da4fc73d605"},"outputs":[{"output_type":"error","ename":"ValidationError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-0ac7f5e0d214>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdavinci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-davinci-003'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"]}],"source":["from langchain.llms import OpenAI\n","\n","davinci = OpenAI(model_name='text-davinci-003')"]},{"cell_type":"markdown","metadata":{"id":"_NvK4o6SDrs0"},"source":["Alternatively if using Azure OpenAI we do:\n","\n","```python\n","from langchain.llms import AzureOpenAI\n","\n","llm = AzureOpenAI(\n","    deployment_name=\"your-azure-deployment\",\n","    model_name=\"text-davinci-003\"\n",")\n","```"]},{"cell_type":"markdown","metadata":{"id":"SGL2zs3uEVj6"},"source":["We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVSsC3iGEPAp","executionInfo":{"status":"aborted","timestamp":1697273884139,"user_tz":-330,"elapsed":6,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["llm_chain = LLMChain(\n","    prompt=prompt,\n","    llm=davinci\n",")\n","\n","print(llm_chain.run(question))"]},{"cell_type":"markdown","metadata":{"id":"DL-buasOKpKs"},"source":["The same works again for multiple questions using `generate`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMua1MWcKtSx","executionInfo":{"status":"aborted","timestamp":1697273884140,"user_tz":-330,"elapsed":7,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["qs = [\n","    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n","    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n","    {'question': \"Who was the 12th person on the moon?\"},\n","    {'question': \"How many eyes does a blade of grass have?\"}\n","]\n","llm_chain.generate(qs)"]},{"cell_type":"markdown","metadata":{"id":"5dCVVmuXBMpO"},"source":["Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2-es7SgFddS","executionInfo":{"status":"aborted","timestamp":1697273884140,"user_tz":-330,"elapsed":7,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["qs = [\n","    \"Which NFL team won the Super Bowl in the 2010 season?\",\n","    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n","    \"Who was the 12th person on the moon?\",\n","    \"How many eyes does a blade of grass have?\"\n","]\n","print(llm_chain.run(qs))"]},{"cell_type":"markdown","metadata":{"id":"pDWkiEQwBMpP"},"source":["Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbjxnnVzA47s","executionInfo":{"status":"aborted","timestamp":1697273884140,"user_tz":-330,"elapsed":7,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"outputs":[],"source":["multi_template = \"\"\"Answer the following questions one at a time.\n","\n","Questions:\n","{questions}\n","\n","Answers:\n","\"\"\"\n","long_prompt = PromptTemplate(\n","    template=multi_template,\n","    input_variables=[\"questions\"]\n",")\n","\n","llm_chain = LLMChain(\n","    prompt=long_prompt,\n","    llm=davinci\n",")\n","\n","qs_str = (\n","    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n","    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n","    \"Who was the 12th person on the moon?\" +\n","    \"How many eyes does a blade of grass have?\"\n",")\n","\n","print(llm_chain.run(qs_str))"]},{"cell_type":"code","source":[],"metadata":{"id":"NA1XikiLJ-V_","executionInfo":{"status":"aborted","timestamp":1697273884140,"user_tz":-330,"elapsed":7,"user":{"displayName":"shrinjita paul","userId":"14845954806627463414"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1r9WU78z2HxPCeGSzjfLTi8T1R8HHHyb-","timestamp":1697273686768},{"file_id":"1Q3pddDdzWs1LOAPEH3FTOO0_I-gtIV-1","timestamp":1697273244869},{"file_id":"https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb","timestamp":1697217052088}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}